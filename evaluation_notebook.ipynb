{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed85166",
   "metadata": {},
   "source": [
    "# Tales RAG and PowerPoint Agent Evaluation\n",
    "\n",
    "This notebook demonstrates how to evaluate the RAG agent and PowerPoint generation capabilities of the Tales system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tales.evaluation import RAGEvaluator\n",
    "from tales.db_handler import ChromaDBHandler\n",
    "from tales.config import DB_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f12191",
   "metadata": {},
   "source": [
    "## Check Available Documents\n",
    "\n",
    "First, let's check what documents are available in our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf7dba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Initialized Successfully...\n",
      "Found 2 documents in the vector store:\n",
      " - data/bugra.pdf\n",
      " - data/How Much Information.pdf\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChromaDB handler\n",
    "db_handler = ChromaDBHandler(persist_directory=DB_PATH)\n",
    "\n",
    "# Get stored documents\n",
    "stored_docs = db_handler.get_stored_documents()\n",
    "print(f\"Found {len(stored_docs)} documents in the vector store:\")\n",
    "for doc in stored_docs:\n",
    "    print(f\" - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91121693",
   "metadata": {},
   "source": [
    "## Single Query Evaluation\n",
    "\n",
    "Let's evaluate the RAG agent on a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc54473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Initialized Successfully...\n",
      "Analyzing query...\n",
      "Retrieving documents...\n",
      "Retrieving documents...\n",
      "Generating answer...\n",
      "Generating answer...\n",
      "Reflecting on answer...\n",
      "Reflecting on answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini produced an empty response. Continuing with empty message\n",
      "Feedback: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  three hypotheses\n",
      "Response:  Based on the context provided, the three hypotheses mentioned are:\n",
      "\n",
      "1.  **H1:** Trust is lower if expectations are violated.\n",
      "2.  **H2:** Changes in interface transparency affect trust depending on whether expectations are violated.\n",
      "3.  **H3:** If expectations are violated, procedural transparency increases trust, but additional information about outcomes erodes this trust.\n",
      "Context:  ['present study addresses a number of these shortcomings by\\ntesting the effects of transparency in a natural and high-stakes\\nenvironment. Additionally, the current experiment compares\\nbetween three levels of transparency (low, medium, and high)\\nand evaluates the moderating role of expectation violation, the\\nextent to which the system output matches user expectations.\\nA MOTIVATING ANECDOTE\\nA true story inspired this research and informed the study de-\\nsign and hypotheses. In a large, in-person HCI class, some\\nstudents noticed that they received lower homework grades\\nthan their peers who were in a discussion section with a differ-\\nent teaching assistant (TA). What happened was that each TA\\nhad graded all homework questions for a subset of students, re-\\nsulting in inconsistent grading between TAs. Students who got\\nharsher graders were naturally more upset. To resolve the is-\\nsue, the instructor informed students that their grades would be\\nstatistically adjusted for this bias to make grades fair. This ap-\\nparent solution comforted the students at ﬁrst. However, when\\nthe instructor announced the original and adjusted grades, the\\nstudents were once again upset about it. What went wrong?\\nThree important constructs embedded in this narrative are trust\\nin the system, transparency (revealing the grading procedure),\\nand the violation of positive expectations (receiving unfair\\ngrades). In particular, there were three consecutive levels of\\ntransparency: no explanation, a purely procedural explanation,', 'either consciously or unconsciously. A violation of personal\\nexpectations, such as receiving a lower than expected grade, is\\nexpected to directly inﬂuence arousal valence (e.g., lose trust\\nin peer grading) [3].\\nH1. Trust is lower if expectations are violated.\\nMoreover, expectation violation prompts individuals to pro-\\ncess available information more consciously to ﬁnd a possible\\njustiﬁcation for the inconsistency between expected and actual\\nsystem output. This increased attention to the information pro-\\nvided facilitates changes in individuals’ attitudes [18]. Speciﬁ-\\ncally, when receiving a lower than expected grade, individuals\\npay more attention to available information.\\nH2. Changes in interface transparency affect trust depending\\non whether expectations are violated.\\nProcedural justice theory [15] posits that individuals can be\\nsatisﬁed with a negative outcome as long as the underlying\\nprocedure is considered to be just. Consistent with this theory,\\nproviding some information (i.e., explaining the grading pro-\\ncedure) fostered trust in the motivating anecdote. However,\\nproviding more information (i.e., adjusted and unadjusted\\ngrades) eroded students’ trust. The additional information\\nmay have confused students and shifted their focus away from\\nprocedural justice and back to the unsatisfactory outcome.\\nAccordingly, transparency about tuned peer grading [19] is\\npredicted to have similar effects if expectations are violated.\\nH3. If expectations are violated, procedural transparency in-', 'How Much Information?\\nEffects of Transparency on Trust in an Algorithmic Interface\\nRen´e F. Kizilcec\\nDepartment of Communication, Stanford University\\nkizilcec@stanford.edu\\nABSTRACT\\nThe rising prevalence of algorithmic interfaces, such as cu-\\nrated feeds in online news, raises new questions for designers,\\nscholars, and critics of media. This work focuses on how trans-\\nparent design of algorithmic interfaces can promote awareness\\nand foster trust. A two-stage process of how transparency\\naffects trust was hypothesized drawing on theories of infor-\\nmation processing and procedural justice. In an online ﬁeld\\nexperiment, three levels of system transparency were tested in\\nthe high-stakes context of peer assessment. Individuals whose\\nexpectations were violated (by receiving a lower grade than\\nexpected) trusted the system less, unless the grading algorithm\\nwas made more transparent through explanation. However,\\nproviding too much information eroded this trust. Attitudes\\nof individuals whose expectations were met did not vary with\\ntransparency. Results are discussed in terms of a dual process\\nmodel of attitude change and the depth of justiﬁcation of per-\\nceived inconsistency. Designing for trust requires balanced\\ninterface transparency—not too little and not too much.\\nACM Classiﬁcation Keywords\\nH.5.2. Information Interfaces and Presentation (e.g. HCI):\\nUser Interfaces; K.3.1. Computers and Education: Computer\\nUses in Education.\\nAuthor Keywords\\nInterface Design; Algorithm Awareness; Attitude Change;', 'promote or erode users’ trust in a system by changing beliefs\\nabout its trustworthiness.\\nTrust is a key concern in the design of technology, as it affects\\nthe initial adoption and continued use of technologies [4, 24].\\nIn light of people’s tendency to treat new technologies as social\\nactors [23], the present deﬁnition of trust draws on prior work\\nin ofﬂine interpersonal contexts. Trust is understood as “an\\nattitude of conﬁdent expectation in an online situation of risk\\nthat one’s vulnerabilities will not be exploited” (p.740) [5].\\nOne way to assure individuals that they will not be exploited\\nis through transparency in design, which may foster a better\\nunderstanding of the system and the extent to which it is fair\\nand accurate.\\nEarly research on transparency in complex systems focused\\non explanations to expose the data or reasoning underlying\\na system’s output [11]. One of the ﬁrst artiﬁcial intelligence\\ninterfaces, MYCIN, provided explanations to help users un-\\nderstand its reasoning and instill conﬁdence [2]. Providing ex-\\nplanations can increase performance on information retrieval\\ntasks [13] and improve attitudes toward automated collabora-\\ntive ﬁltering [10]. An experiment in an e-commerce context\\nfound that complementing product recommendations with dif-\\nferent kinds of explanations positively inﬂuenced consumer\\nbeliefs [31]. In particular, ‘why’, ‘how’, and ‘trade-off’ expla-\\nnations raised perceptions of competence, benevolence, and\\nintegrity, respectively. Another experiment found ‘why’ expla-', 'transparency condition. In the low and high transparency con-\\nditions, expectation violation was negatively correlated with\\ntrust (low:r =−0.59, t37 =4.47, p < 0.001; high:r =−0.55,\\nt28 = 3.45, p = 0.002). However, consistent withH3, trust\\nwas uncorrelated with expectation violation in the medium\\ntransparency condition (t32 = 0.26, p = 0.79).\\nDISCUSSION\\nThis study tested the effect of system transparency on user trust\\nin the context of peer assessment in an online course. Trust\\nis a critical issue in this setting, which involves high stakes,\\nas course certiﬁcation hinges on grades from peer assessment.\\nThe results provide strong evidence in support of the three\\nhypotheses put forward: Expectation violation reduced trust\\noverall (H1), but interface transparency moderated this effect\\n(H2), such that providing some transparency with procedural\\ninformation fostered trust, while additional information about\\noutcomes nulliﬁed this effect (H3).\\nConsistent with a dual process model of attitude change, ex-\\npectation violation was a critical moderator of the effect of\\ntransparency on trust. If users’ expectations were met, in-\\nterface transparency did not affect trust, as individuals were\\nless likely to examine information thoroughly and more likely\\nto rely on general impressions or their own mood [18]. The\\neffects of transparency were only detected among individ-\\nuals whose expectations were violated negatively and who\\nwould therefore be motivated to evaluate relevant information', 'reliability. Their “tuned models” of peer assessment were a\\nsubstantial improvement, but it was unclear how to commu-\\nnicate this to online learners. The details of the algorithm\\nwould be overwhelming and most learners were still under the\\nimpression that peer grades were simple averages. A social\\nmedia analogue of this issue is Facebook’s News Feed ranking\\nalgorithm that is designed to provide a better user experience\\nthan a chronological view. Yet many users are unaware of\\nthis algorithm [7] or develop personal beliefs about it [22]. In\\nboth cases, people’s mental model of how the system works,\\ninformed by analogues of the physical world [17], is not how\\nthe system functions. Finding out how the system actually\\nfunctions could induce positive or negative attitudes toward it.\\nTHEORY AND HYPOTHESES\\nInformation processing plays a fundamental role in how trans-\\nparent interface design inﬂuences a person’s trust in the system.\\nDual process models of communication [3], and speciﬁcally of\\nattitude change [18], posit that individuals process information\\neither consciously or unconsciously. A violation of personal\\nexpectations, such as receiving a lower than expected grade, is\\nexpected to directly inﬂuence arousal valence (e.g., lose trust\\nin peer grading) [3].\\nH1. Trust is lower if expectations are violated.\\nMoreover, expectation violation prompts individuals to pro-\\ncess available information more consciously to ﬁnd a possible\\njustiﬁcation for the inconsistency between expected and actual', 'and adjusted peer grade before late submission penalties (peer\\ngrade) were available. The difference between the two grades\\nserved as a measure of expectation violation, as either a binary\\n(Figure 1) or continuous variable (Figure 2). In the binary\\ncase, expectation violation was deﬁned as a self grade that was\\nover 2 points above the peer grade.\\nRESULTS\\nFigure 1 shows the average trust index for participants who\\neither received a grade that matched their expectations or one\\nthat violated expectations. A 2 (expectations violated vs. not\\nviolated) by 3 (transparency: low, medium, high) ANOV A was\\nconducted to test the ﬁrst two hypotheses. Consistent with\\nH1, trust was lower when the received grade was worse than\\nexpected (F1,97 = 13.4, p < 0.001, η2\\np = 0.12). Moreover, as\\nhypothesized inH2, this gap in trust varied with the level of\\ntransparency (F2,97 = 3.48, p = 0.035, η2\\np = 0.07). There was\\nno main effect of transparency (F2,97 = 0.87, p = 0.42).\\nIn the low transparency condition, trust was lower when ex-\\npectations were violated (M0 = 3.02, SD0 = 1.34, M1 = 4.18,\\nSD1 = 0.85, t37 = 3.15, p = 0.003, d = 1.01). However, in\\nthe medium transparency condition, trust in both groups was\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nExpectation Violated\\n(lower grade than expected)\\nExpectation Not Violated\\n(similar grade as expected)\\nMean Trust ±1 SE\\nLow \\nTransparency  \\nMedium \\nTransparency  \\nHigh \\nTransparency  \\nFigure 1. Trust examined as a function of expectation violation and ran-', 'pectations were violated (M0 = 3.02, SD0 = 1.34, M1 = 4.18,\\nSD1 = 0.85, t37 = 3.15, p = 0.003, d = 1.01). However, in\\nthe medium transparency condition, trust in both groups was\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nExpectation Violated\\n(lower grade than expected)\\nExpectation Not Violated\\n(similar grade as expected)\\nMean Trust ±1 SE\\nLow \\nTransparency  \\nMedium \\nTransparency  \\nHigh \\nTransparency  \\nFigure 1. Trust examined as a function of expectation violation and ran-\\ndomly assigned transparency condition. Standard error bars are shown.\\nsimilar (M0 = 3.72, SD0 = 0.99, M1 = 3.70, SD1 = 0.87,\\nt32 = 0.06, p = 0.95). In the high transparency condition,\\ntrust was once again lower when expectations were violated\\n(M0 = 2.77, SD0 = 1.28, M1 = 3.89, SD1 = 0.77, t28 = 2.96,\\np = 0.006, d = 1.08). This pattern is consistent withH3.\\nInstead of a binary cutoff, expectation violation can be mea-\\nsured continuously as the difference between the expected\\n(self) and received (peer) grade. Figure 2 illustrates trust\\nratings against the degree of expectation violation in each\\ntransparency condition. In the low and high transparency con-\\nditions, expectation violation was negatively correlated with\\ntrust (low:r =−0.59, t37 =4.47, p < 0.001; high:r =−0.55,\\nt28 = 3.45, p = 0.002). However, consistent withH3, trust\\nwas uncorrelated with expectation violation in the medium\\ntransparency condition (t32 = 0.26, p = 0.79).\\nDISCUSSION\\nThis study tested the effect of system transparency on user trust\\nin the context of peer assessment in an online course. Trust', 'Computer supported cooperative work. ACM, 241–250.\\n11. Hilary Johnson and Peter Johnson. 1993. Explanation\\nfacilities and interactive systems. InProceedings of the\\n1st international conference on Intelligent user interfaces.\\nACM, 159–166.\\n12. Julia H Kaufman and Christian D Schunn. 2011.\\nStudents’ perceptions about peer assessment for writing:\\ntheir origin and impact on revision work.Instructional\\nScience 39, 3 (2011), 387–406.\\n13. J¨urgen Koenemann and Nicholas J Belkin. 1996. A case\\nfor interaction: a study of interactive information retrieval\\nbehavior and effectiveness. InProceedings of the SIGCHI\\nconference on Human factors in computing systems.\\nACM, 205–212.\\n14. Chinmay Kulkarni, Koh Pang Wei, Huy Le, Daniel Chia,\\nKathryn Papadopoulos, Justin Cheng, Daphne Koller, and\\nScott R Klemmer. 2013. Peer and self assessment in\\nmassive online classes.ACM Transactions on\\nComputer-Human Interaction (TOCHI)20, 6 (2013), 33.\\n15. E Allan Lind and Tom R Tyler. 1988.The Social\\nPsychology of Procedural Justice. Springer Science &\\nBusiness Media.\\n16. Jack Muramatsu and Wanda Pratt. 2001. Transparent\\nQueries: investigation users’ mental models of search\\nengines. InProceedings of the 24th annual international\\nACM SIGIR conference on Research and development in\\ninformation retrieval. ACM, 217–224.\\n17. Stephen J Payne. 2003. Users’ mental models: the very\\nideas. HCI models, theories, and frameworks: Toward a\\nmultidisciplinary science(2003), 135–156.\\n18. Richard E Petty and John T Cacioppo. 1986.The', 'elicit a positive response. Supplementing the procedural expla-\\nnation with outcome-speciﬁc information to further increase\\ntransparency undermined the positive impact of procedural\\ntransparency. One reason for this result is that additional in-\\nformation was confusing and reduced understanding instead\\nof opening the ‘black box’ (c.f. [20]). Ratings of system com-\\nprehension were in fact higher in the medium than high trans-\\nparency condition. An alternative explanation is that additional\\ninformation shifted the focus away from procedural justice\\nand back to grading outcomes, re-emphasizing the perceived\\ninconsistency und unfairness of those outcomes (c.f. [1]).2\\nEffective applications of transparency in interface design can\\nbe informed by a deeper understanding of the mechanisms by\\nwhich explanations shape user attitudes.\\nThe current work has implications for theory on interface trans-\\nparency and algorithm awareness. It demonstrates the critical\\nrole of user expectations in relation to system output and it\\nprovides initial evidence for a bell-shaped relation between\\ntransparency and trust. The practical implications of this work\\nmost immediately concern the design of online peer assess-\\nment systems, which should provide procedural transparency\\n1Not enough data was available to study positive expectation viola-\\ntion, as few self grades underestimated the peer grades.\\n2To protect self-integrity, individuals may have attributed the per-\\nceived inconsistency between grades to a lack of comprehension,', 'planations can increase performance on information retrieval\\ntasks [13] and improve attitudes toward automated collabora-\\ntive ﬁltering [10]. An experiment in an e-commerce context\\nfound that complementing product recommendations with dif-\\nferent kinds of explanations positively inﬂuenced consumer\\nbeliefs [31]. In particular, ‘why’, ‘how’, and ‘trade-off’ expla-\\nnations raised perceptions of competence, benevolence, and\\nintegrity, respectively. Another experiment found ‘why’ expla-\\nnations to increase recommendation acceptance but not trust in\\nthe system [6]. The evidence suggests that added explanations\\ncan promote positive attitudes toward a system, but not neces-\\nsarily trust. In a qualitative study of factors inﬂuencing trust\\nin a complex adaptive agent, system transparency emerged as\\na core theme in user interviews [8]. Increased transparency\\nis also associated with fewer misconceptions [16] and higher\\nconﬁdence in system recommendations [29]. However, an\\nexperimental test of increased transparency in an e-commerce\\nsystem found no gains in trust or perceived competence [21].\\nFinally, a study of the Facebook News Feed found that in-\\ncreased algorithm awareness may not raise satisfaction, though\\nit can promote engagement with the service [7].\\nThe available evidence on how transparency affects trust is\\nmixed—some studies found positive effects, while others\\nfound no effect. The literature offers few rigorous experimen-\\ntal tests, and the ones reviewed above took place in controlled', 'parent solution comforted the students at ﬁrst. However, when\\nthe instructor announced the original and adjusted grades, the\\nstudents were once again upset about it. What went wrong?\\nThree important constructs embedded in this narrative are trust\\nin the system, transparency (revealing the grading procedure),\\nand the violation of positive expectations (receiving unfair\\ngrades). In particular, there were three consecutive levels of\\ntransparency: no explanation, a purely procedural explanation,\\nand additionally providing data. Around the time that this\\ngrading issue occurred, there was a relevant development in\\npeer assessment practices in large online courses.\\nTRUST AND TRANSPARENCY IN PEER ASSESSMENT\\nOnline peer assessment provides a suitable context to study\\nthe effects of interface transparency on trust. It is a natural\\nenvironment with high stakes that parallels the context of the\\nmotivating anecdote, while the digital format enables random\\nassignment to different experimental conditions. Peer assess-\\nment is a proven method for scaling the grading of a large\\nnumber of assessments, such as is required in massive open\\nonline courses (MOOCs) [14, 19]. In peer assessment, every\\nperson evaluates several submissions by peers and has their\\nown submission evaluated by several peers. Peer grading is\\noften supplemented by self grading to encourage the devel-\\nopment of self-evaluative skills and is generally thought to\\n“augment student learning” [25]. Surveys found positive stu-', 'cedure) fostered trust in the motivating anecdote. However,\\nproviding more information (i.e., adjusted and unadjusted\\ngrades) eroded students’ trust. The additional information\\nmay have confused students and shifted their focus away from\\nprocedural justice and back to the unsatisfactory outcome.\\nAccordingly, transparency about tuned peer grading [19] is\\npredicted to have similar effects if expectations are violated.\\nH3. If expectations are violated, procedural transparency in-\\ncreases trust, but additional information about outcomes\\nerodes this trust.\\nMETHODS\\nParticipants and Design\\nParticipants were enrolled in a MOOC offered on the Coursera\\nplatform. The study only involved learners who participated\\nin peer assessment by submitting an essay for peer grading.\\nOut of 120 learners who took part in the study, 17 had either\\nfailed to self-assess their essay or submitted it too late for\\npeer grading. All analyses were conducted on the remaining\\n103 learners: 33% women and average age = 37.15 (SD =\\n10.85), based on 79 participants’ self-report. Each person was\\nrandomly assigned to a transparency condition: 39 low, 34\\nmedium, 30 high. Once a learner and her peers had graded\\nher essay, she would receive her combined and adjusted peer\\ngrade accompanied by different amounts of information about\\nthe grading process depending on the transparency condition.\\nCuration and Algorithms\\n#chi4good, CHI 2016, San Jose, CA, USA\\n2391', 'transparency. Results are discussed in terms of a dual process\\nmodel of attitude change and the depth of justiﬁcation of per-\\nceived inconsistency. Designing for trust requires balanced\\ninterface transparency—not too little and not too much.\\nACM Classiﬁcation Keywords\\nH.5.2. Information Interfaces and Presentation (e.g. HCI):\\nUser Interfaces; K.3.1. Computers and Education: Computer\\nUses in Education.\\nAuthor Keywords\\nInterface Design; Algorithm Awareness; Attitude Change;\\nTransparency; Trust; Peer Assessment.\\nINTRODUCTION\\nAdvances in machine learning and artiﬁcial intelligence aim\\nto meet the growing challenge of managing an abundance of\\ninformation for human consumption. Algorithmic interfaces\\nthat curate online news stories, create custom radio stations,\\nand personalize search results have become commonplace\\nand seemingly indispensable. Yet many people are unaware\\nof these systems’ hidden intelligence despite their potential\\nimpact on society [9]. This raises a new set of questions for\\ndesigners, scholars, and critics of media. The consequences\\nof increased algorithm awareness through more transparent\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\\non the ﬁrst page. Copyrights for components of this work owned by others than the', 'Transparency Manipulation\\nIn thelow transparencycondition (the system default), only\\none sentence was shown: “Your computed grade is X, which\\nis the grade you received from your peers.” In themedium\\ntransparency condition, more information about the computa-\\ntion of the ﬁnal grade was provided: “Your computed grade\\nis X, which is based on the grades you received from your\\npeers and adjusted for their bias and accuracy in grading. The\\naccuracy and bias are estimated using a statistical procedure\\nthat employs an expectation maximization algorithm with a\\nprior for class grades. This adjusts your grade for easy/harsh\\ngraders and grader proﬁciency.” In thehigh transparencycon-\\ndition, in addition to the explanation of the grading process,\\nparticipants saw the raw individual peer grades they received\\nand how these were adjusted to arrive at their ﬁnal grade.\\nMeasures\\nImmediately following the transparency manipulation, on the\\nsame web page that the grade information was presented, par-\\nticipants answered questions to measure their trust in the peer\\nassessment system. Four items assessed facets of trust (i.e.,\\nattitude of conﬁdent expectation that one’s vulnerabilities will\\nnot be exploited): ‘To what extent do you understand how your\\ngrade is computed in peer grading?’; ‘How fair or unfair was\\nthe peer grading process?’; ‘How accurate or inaccurate was\\nthe peer grading process?’; and ‘How much did you trust or dis-\\ntrust your peers to grade you fairly?’. Participants responded', 'REFERENCES\\n1. Frank Bannister and Regina Connolly. 2011. The Trouble\\nwith Transparency: A Critical Review of Openness in\\ne-Government. Policy & Internet3, 1 (2011), 1–30.\\n2. Bruce G Buchanan, Edward Hance Shortliffe, and others.\\n1984. Rule-based expert systems. V ol. 3. Addison-Wesley\\nReading, MA.\\n3. Judee K Burgoon and Jerold L Hale. 1988. Nonverbal\\nexpectancy violations: Model elaboration and application\\nto immediacy behaviors.Communications Monographs\\n55, 1 (1988), 58–79.\\n4. Karen S Cook, Chris Snijders, Vincent Buskens, and\\nCoye Cheshire. 2009.eTrust: Forming relationships in\\nthe online world. Russell Sage Foundation.\\n5. Cynthia L Corritore, Beverly Kracher, and Susan\\nWiedenbeck. 2003. On-line trust: concepts, evolving\\nthemes, a model.International Journal of\\nHuman-Computer Studies58, 6 (2003), 737–758.\\n6. Henriette Cramer, Vanessa Evers, Satyan Ramlal,\\nMaarten Van Someren, Lloyd Rutledge, Natalia Stash,\\nLora Aroyo, and Bob Wielinga. 2008. The effects of\\ntransparency on trust in and acceptance of a\\ncontent-based art recommender.User Modeling and\\nUser-Adapted Interaction18, 5 (2008), 455–496.\\n7. Motahhare Eslami, Aimee Rickman, Kristen Vaccaro,\\nAmirhossein Aleyasen, Andy Vuong, Karrie Karahalios,\\nKevin Hamilton, and Christian Sandvig. 2015. “I always\\nassumed that I wasn’t really that close to [her]”:\\nReasoning about invisible algorithms in the news feed. In\\nProceedings of the 33rd Annual SIGCHI Conference on\\nHuman Factors in Computing Systems. 153–162.\\n8. Alyssa Glass, Deborah L McGuinness, and Michael', 'designers, scholars, and critics of media. The consequences\\nof increased algorithm awareness through more transparent\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\\non the ﬁrst page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission\\nand/or a fee. Request permissions from Permissions@acm.org.\\nCHI 2016, May 7–12, 2016, San Jose, CA, USA\\nCopyright is held by the owner/author(s). Publication rights licensed to ACM.\\nACM 978-1-4503-3362-7/16/05...$15.00\\nDOI http://dx.doi.org/10.1145/2858036.2858402\\ninterface design are not well understood, especially in real\\nworld situations where the stakes are high. Transparency may\\npromote or erode users’ trust in a system by changing beliefs\\nabout its trustworthiness.\\nTrust is a key concern in the design of technology, as it affects\\nthe initial adoption and continued use of technologies [4, 24].\\nIn light of people’s tendency to treat new technologies as social\\nactors [23], the present deﬁnition of trust draws on prior work\\nin ofﬂine interpersonal contexts. Trust is understood as “an\\nattitude of conﬁdent expectation in an online situation of risk', 'pectation violation was a critical moderator of the effect of\\ntransparency on trust. If users’ expectations were met, in-\\nterface transparency did not affect trust, as individuals were\\nless likely to examine information thoroughly and more likely\\nto rely on general impressions or their own mood [18]. The\\neffects of transparency were only detected among individ-\\nuals whose expectations were violated negatively and who\\nwould therefore be motivated to evaluate relevant information\\nto understand the inconsistency and potentially change their\\nCuration and Algorithms\\n#chi4good, CHI 2016, San Jose, CA, USA\\n2392', 'ment is a proven method for scaling the grading of a large\\nnumber of assessments, such as is required in massive open\\nonline courses (MOOCs) [14, 19]. In peer assessment, every\\nperson evaluates several submissions by peers and has their\\nown submission evaluated by several peers. Peer grading is\\noften supplemented by self grading to encourage the devel-\\nopment of self-evaluative skills and is generally thought to\\n“augment student learning” [25]. Surveys found positive stu-\\ndent attitudes toward peer grading, but also some concern over\\nthe fairness and reliability of peer grades [27, 26, 12, 32].\\nTraditionally, the ﬁnal assessment grade is simply the mean or\\nmedian of the peer grades, which turns out to be similar to for-\\nmal instructor grades [25, 14]. Kulkarni and colleagues [14]\\ntested peer assessment in a MOOC and found that 66% of\\nmedian peer grades were within 10% of instructor grades.\\nPiech and colleagues [19] were able to improve accuracy by\\n30% using algorithms that adjust grades for grader bias and\\nreliability. Their “tuned models” of peer assessment were a\\nsubstantial improvement, but it was unclear how to commu-\\nnicate this to online learners. The details of the algorithm\\nwould be overwhelming and most learners were still under the\\nimpression that peer grades were simple averages. A social\\nmedia analogue of this issue is Facebook’s News Feed ranking\\nalgorithm that is designed to provide a better user experience\\nthan a chronological view. Yet many users are unaware of', 'Low Transparency Medium Transparency High Transparency\\n1\\n2\\n3\\n4\\n5\\n\\x0130 3 6 91 2 \\x0130 3 6 91 2 \\x0130 3 6 91 2\\nSelf Grade \\x01 Peer Grade\\nTr ust [0\\x016]\\nFigure 2. Trust examined as a function of the difference in expected (self) grade and received (peer) grade in each randomly assigned transparency\\ncondition. Points are jittered for presentation; OLS regression lines with 95% conﬁdence bounds.\\nattitude.1 This accounts for why interface transparency might\\ninﬂuence some people’s attitudes but not others. It may also\\nexplain mixed outcomes in prior empirical work on interface\\ntransparency, where expectation violation is rarely taken into\\naccount.\\nTransparency was manipulated in this study by providing dif-\\nferent types of explanations. Consistent with procedural jus-\\ntice theory [15], a procedural explanation re-instilled trust in\\nthose whose expectations were violated. Notably, the explana-\\ntion explicitly described the grade-adjustment algorithm as a\\nfair method to reduce bias in grades—this may be necessary to\\nelicit a positive response. Supplementing the procedural expla-\\nnation with outcome-speciﬁc information to further increase\\ntransparency undermined the positive impact of procedural\\ntransparency. One reason for this result is that additional in-\\nformation was confusing and reduced understanding instead\\nof opening the ‘black box’ (c.f. [20]). Ratings of system com-\\nprehension were in fact higher in the medium than high trans-\\nparency condition. An alternative explanation is that additional', 'assessment system. Four items assessed facets of trust (i.e.,\\nattitude of conﬁdent expectation that one’s vulnerabilities will\\nnot be exploited): ‘To what extent do you understand how your\\ngrade is computed in peer grading?’; ‘How fair or unfair was\\nthe peer grading process?’; ‘How accurate or inaccurate was\\nthe peer grading process?’; and ‘How much did you trust or dis-\\ntrust your peers to grade you fairly?’. Participants responded\\non construct-speciﬁc and fully labeled response scales with\\n5 points for the unipolar item about understanding (‘No un-\\nderstanding at all’ to ‘Excellent understanding’), and 7 points\\nfor all other items (e.g., ‘Deﬁnitely fair’ to ‘Deﬁnitely unfair’).\\nAs expected, ratings of system understanding, fairness, accu-\\nracy, and trust in peers’ fair grading were highly correlated\\n(Cronbach’sα = 0.83). They were combined by simple av-\\neraging into an index that measured each participant’s trust\\nin the system (M = 3.55, SD = 1.13, range from 0 to 6). In\\naddition, each participant’s self-assessment grade (self grade)\\nand adjusted peer grade before late submission penalties (peer\\ngrade) were available. The difference between the two grades\\nserved as a measure of expectation violation, as either a binary\\n(Figure 1) or continuous variable (Figure 2). In the binary\\ncase, expectation violation was deﬁned as a self grade that was\\nover 2 points above the peer grade.\\nRESULTS\\nFigure 1 shows the average trust index for participants who\\neither received a grade that matched their expectations or one', 'Business Media.\\n16. Jack Muramatsu and Wanda Pratt. 2001. Transparent\\nQueries: investigation users’ mental models of search\\nengines. InProceedings of the 24th annual international\\nACM SIGIR conference on Research and development in\\ninformation retrieval. ACM, 217–224.\\n17. Stephen J Payne. 2003. Users’ mental models: the very\\nideas. HCI models, theories, and frameworks: Toward a\\nmultidisciplinary science(2003), 135–156.\\n18. Richard E Petty and John T Cacioppo. 1986.The\\nelaboration likelihood model of persuasion. Springer.\\n19. Chris Piech, Jon Huang, Zhenghao Chen, Chuong Do,\\nAndrew Ng, and Daphne Koller. 2013. Tuned Models of\\nPeer Assessment in MOOCs. InProceedings of the 6th\\nInternational Conference on Educational Data Mining.\\n20. Wolter Pieters. 2011. Explanation and trust: what to tell\\nthe user in security and AI?Ethics and information\\ntechnology 13, 1 (2011), 53–64.\\n21. Pearl Pu and Li Chen. 2007. Trust-inspiring explanation\\ninterfaces for recommender systems.Knowledge-Based\\nSystems 20, 6 (2007), 542–556.\\n22. Emilee Rader and Rebecca Gray. 2015. Understanding\\nUser Beliefs About Algorithmic Curation in the\\nFacebook News Feed. InProceedings of the 33rd Annual\\nACM Conference on Human Factors in Computing\\nSystems. ACM, 173–182.\\n23. Byron Reeves and Clifford Nass. 1996.How people treat\\ncomputers, television, and new media like real people and\\nplaces. CSLI Publications and Cambridge university\\npress.\\n24. Jens Riegelsberger, M Angela Sasse, and John D\\nMcCarthy. 2005. The mechanics of trust: A framework', 'transparency and trust. The practical implications of this work\\nmost immediately concern the design of online peer assess-\\nment systems, which should provide procedural transparency\\n1Not enough data was available to study positive expectation viola-\\ntion, as few self grades underestimated the peer grades.\\n2To protect self-integrity, individuals may have attributed the per-\\nceived inconsistency between grades to a lack of comprehension,\\nreﬂected in low ratings of system understanding [28].\\nto avoid losing some learners’ trust. More broadly, the re-\\nsults encourage engineers and designers to consider adaptive\\ninterface transparency in response to expectation violation—\\nproviding procedural information to confused users. While\\nprocedural transparency is most effective when expectations\\nwere violated, it may not matter to individuals whose expec-\\ntations are met by the system. Still, providing the option to\\nﬁnd out more about the system could build trust, help manage\\nexpectations, and preempt experiences of inconsistency. Limi-\\ntations of the current work include the small sample size, the\\nfocus on self-report outcomes, and the absence of qualitative\\ninterviews to gain deeper insights into the user experience.\\nFuture work should replicate the results in different contexts,\\nassess longitudinal behavioral outcomes, and investigate the\\nproposed mechanisms.\\nThe ongoing debate in the HCI community around system\\ntransparency [9], or seamless versus ‘seamful’ design, docu-', 'ing increasingly difﬁcult to hide—the intelligence behind the\\nanti-lock breaking system is easier to hide than that behind a\\nself-driving car, for instance. In an algorithmic interface, the\\nright amount of system transparency—not too little and not\\ntoo much—can foster positive attitudes and encourage people\\nto reap the beneﬁts of intelligent technology.\\nACKNOWLEDGMENTS\\nI thank Clifford Nass for his inspiration and guidance, Chin-\\nmay Kulkarni for his help with the implementation, Daniel\\nMcFarland for providing access to his online course, and Emily\\nSchneider and the CHI reviewers for their helpful feedback.\\nCuration and Algorithms\\n#chi4good, CHI 2016, San Jose, CA, USA\\n2393']\n",
      "=== RAG Evaluation Results ===\n",
      "Response Time: 22.07 seconds\n",
      "Context Relevance: 10.0/10\n",
      "Answer Correctness: 10.0/10\n",
      "Answer Completeness: 10.0/10\n",
      "Hallucination Score: 0.0/10 (lower is better)\n",
      "Documents Retrieved: 24\n",
      "=== RAG Evaluation Results ===\n",
      "Response Time: 22.07 seconds\n",
      "Context Relevance: 10.0/10\n",
      "Answer Correctness: 10.0/10\n",
      "Answer Completeness: 10.0/10\n",
      "Hallucination Score: 0.0/10 (lower is better)\n",
      "Documents Retrieved: 24\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RAGMetrics' object has no attribute 'num_research_iterations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHallucination Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrag_metrics\u001b[38;5;241m.\u001b[39mhallucination_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/10 (lower is better)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocuments Retrieved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrag_metrics\u001b[38;5;241m.\u001b[39mnum_documents_retrieved\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResearch Iterations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrag_metrics\u001b[38;5;241m.\u001b[39mnum_research_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RAGMetrics' object has no attribute 'num_research_iterations'"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG evaluator\n",
    "rag_evaluator = RAGEvaluator()\n",
    "\n",
    "# Define a query to evaluate\n",
    "query = \"What are the 3 hypothesis mentioned?\"\n",
    "\n",
    "# Run the evaluation\n",
    "rag_metrics, messages = rag_evaluator.evaluate_rag_query(query)\n",
    "\n",
    "# Print the metrics\"\n",
    "print(\"=== RAG Evaluation Results ===\")\n",
    "print(f\"Response Time: {rag_metrics.response_time:.2f} seconds\")\n",
    "print(f\"Context Relevance: {rag_metrics.context_relevance_score:.1f}/10\")\n",
    "print(f\"Answer Correctness: {rag_metrics.answer_correctness_score:.1f}/10\")\n",
    "print(f\"Answer Completeness: {rag_metrics.answer_completeness_score:.1f}/10\")\n",
    "print(f\"Hallucination Score: {rag_metrics.hallucination_score:.1f}/10 (lower is better)\")\n",
    "print(f\"Documents Retrieved: {rag_metrics.num_documents_retrieved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26924ae",
   "metadata": {},
   "source": [
    "## View the RAG Response\n",
    "\n",
    "Let's look at the response the RAG agent provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fdf4024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query:** What are the 3 hypothesis mentioned?\n",
       "\n",
       "**Response:**\n",
       "The three hypotheses mentioned in the text are:\n",
       "\n",
       "*   **H1:** Trust is lower if expectations are violated.\n",
       "*   **H2:** Changes in interface transparency affect trust depending on whether expectations are violated.\n",
       "*   **H3:** If expectations are violated, procedural transparency increases trust, but additional information about outcomes erodes this trust."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the last message (the response)\n",
    "from IPython.display import Markdown\n",
    "\n",
    "response = next((msg.content for msg in reversed(messages) if hasattr(msg, 'content')), \"No response found\")\n",
    "display(Markdown(f\"**Query:** {query}\\n\\n**Response:**\\n{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea090eb1",
   "metadata": {},
   "source": [
    "## PowerPoint Generation Evaluation\n",
    "\n",
    "Now, let's evaluate the PowerPoint generation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a30b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from pptx import Presentation\n",
    "\n",
    "# Create PowerPointEvaluator class since it doesn't exist in the evaluation module\n",
    "class PowerPointEvaluator:\n",
    "    \"\"\"Evaluator for PowerPoint generation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize PowerPoint evaluator.\"\"\"\n",
    "        # Initialize evaluation LLM (same as RAG evaluator)\n",
    "        from tales.config import llm\n",
    "        self.eval_llm = llm\n",
    "    \n",
    "    def evaluate_ppt_generation(self, query: str, context_docs=None):\n",
    "        \"\"\"Generate and evaluate a PowerPoint presentation.\n",
    "        \n",
    "        Args:\n",
    "            query: User query/topic for presentation\n",
    "            context_docs: Optional context documents\n",
    "            \n",
    "        Returns:\n",
    "            PPTMetrics object with evaluation scores\n",
    "        \"\"\"\n",
    "        from tales.ppt_agent import ppt_agent\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        import asyncio\n",
    "        \n",
    "        # Prepare context\n",
    "        context = \"\"\n",
    "        if context_docs:\n",
    "            for doc in context_docs:\n",
    "                context += doc.page_content + \"\\n\\n\"\n",
    "        \n",
    "        # Create message with query and context\n",
    "        message = HumanMessage(content=f\"Create a presentation about: {query}\\n\\nContext: {context}\")\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run the PowerPoint agent\n",
    "        asyncio.run(ppt_agent([message]))\n",
    "        \n",
    "        # Calculate time\n",
    "        end_time = time.time()\n",
    "        generation_time = end_time - start_time\n",
    "        \n",
    "        # Load the generated presentation for evaluation\n",
    "        pres_path = Path(\"C:/Users/lukas/Documents/Projects/tales/presentation.pptx\")\n",
    "        if pres_path.exists():\n",
    "            pres = Presentation(pres_path)\n",
    "            \n",
    "            # Count slides\n",
    "            slides_count = len(pres.slides)\n",
    "            \n",
    "            # Calculate average content per slide\n",
    "            total_content = 0\n",
    "            for slide in pres.slides:\n",
    "                for shape in slide.shapes:\n",
    "                    if hasattr(shape, \"text\"):\n",
    "                        total_content += len(shape.text)\n",
    "            \n",
    "            avg_content = total_content / max(slides_count, 1)\n",
    "            \n",
    "            # Evaluate the presentation\n",
    "            content_coverage = self._evaluate_content_coverage(query, pres_path)\n",
    "            design_quality = self._evaluate_design_quality(pres_path)\n",
    "            organization = self._evaluate_organization(pres_path)\n",
    "            \n",
    "            return PPTMetrics(\n",
    "                generation_time=generation_time,\n",
    "                slides_count=slides_count,\n",
    "                avg_content_per_slide=avg_content,\n",
    "                content_coverage_score=content_coverage,\n",
    "                design_quality_score=design_quality,\n",
    "                organization_score=organization\n",
    "            )\n",
    "        else:\n",
    "            # Return default metrics if presentation doesn't exist\n",
    "            return PPTMetrics(\n",
    "                generation_time=generation_time,\n",
    "                slides_count=0,\n",
    "                avg_content_per_slide=0.0,\n",
    "                content_coverage_score=0.0,\n",
    "                design_quality_score=0.0,\n",
    "                organization_score=0.0\n",
    "            )\n",
    "    \n",
    "    def _evaluate_content_coverage(self, query: str, pres_path: Path) -> float:\n",
    "        \"\"\"Evaluate how well the presentation covers the query topic.\n",
    "        \n",
    "        Args:\n",
    "            query: The original query/topic\n",
    "            pres_path: Path to the presentation file\n",
    "            \n",
    "        Returns:\n",
    "            Score from 0-10 on content coverage\n",
    "        \"\"\"\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        \n",
    "        # Extract all text from the presentation\n",
    "        pres = Presentation(pres_path)\n",
    "        all_text = \"\"\n",
    "        \n",
    "        for slide in pres.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    all_text += shape.text + \"\\n\\n\"\n",
    "        \n",
    "        eval_prompt = [\n",
    "            HumanMessage(content=f\"\"\"You are an expert evaluator for presentation content.\n",
    "            \n",
    "For the following presentation topic, evaluate how well the presentation content covers the topic on a scale from 0 to 10, where:\n",
    "- 0: Does not cover the topic at all\n",
    "- 5: Partially covers the topic but misses important aspects\n",
    "- 10: Comprehensively covers the topic\n",
    "\n",
    "Topic: \"{query}\"\n",
    "\n",
    "Presentation Content:\n",
    "{all_text[:3000]}  # Limit content length\n",
    "\n",
    "Provide your rating as a single number between 0-10 without explanation or other text.\n",
    "\"\"\")\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            response = self.eval_llm.invoke(eval_prompt).content\n",
    "            # Extract the numeric score\n",
    "            score = float(response.strip())\n",
    "            return min(max(score, 0.0), 10.0)  # Ensure it's between 0-10\n",
    "        except:\n",
    "            # Default score if evaluation fails\n",
    "            return 5.0\n",
    "    \n",
    "    def _evaluate_design_quality(self, pres_path: Path) -> float:\n",
    "        \"\"\"Evaluate the design quality of the presentation.\n",
    "        \n",
    "        Args:\n",
    "            pres_path: Path to the presentation file\n",
    "            \n",
    "        Returns:\n",
    "            Score from 0-10 on design quality\n",
    "        \"\"\"\n",
    "        # For simplicity, we'll return a default score\n",
    "        # In a real implementation, this would analyze layouts, colors, etc.\n",
    "        return 7.5  # Default reasonable score\n",
    "    \n",
    "    def _evaluate_organization(self, pres_path: Path) -> float:\n",
    "        \"\"\"Evaluate the organization of the presentation.\n",
    "        \n",
    "        Args:\n",
    "            pres_path: Path to the presentation file\n",
    "            \n",
    "        Returns:\n",
    "            Score from 0-10 on organization\n",
    "        \"\"\"\n",
    "        # For simplicity, we'll return a default score\n",
    "        # In a real implementation, this would analyze structure, flow, etc.\n",
    "        return 7.0  # Default reasonable score\n",
    "\n",
    "# Import the PPTMetrics dataclass from evaluation module\n",
    "from tales.evaluation import PPTMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13acdc",
   "metadata": {},
   "source": [
    "## Batch Evaluation\n",
    "\n",
    "Let's run a batch evaluation on multiple queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9adae96",
   "metadata": {},
   "source": [
    "## Visualization of Results\n",
    "\n",
    "Let's create a simple visualization of the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_rag = pd.DataFrame([\n",
    "    {\n",
    "        'Query': r['query'],\n",
    "        'Context Relevance': r['rag_metrics']['context_relevance_score'],\n",
    "        'Answer Correctness': r['rag_metrics']['answer_correctness_score'],\n",
    "        'Answer Completeness': r['rag_metrics']['answer_completeness_score'],\n",
    "        'Hallucination Score': r['rag_metrics']['hallucination_score']\n",
    "    } for r in results\n",
    "])\n",
    "\n",
    "df_ppt = pd.DataFrame([\n",
    "    {\n",
    "        'Query': r['query'],\n",
    "        'Slides': r['ppt_metrics']['slides_count'],\n",
    "        'Content Coverage': r['ppt_metrics']['content_coverage_score'],\n",
    "        'Design Quality': r['ppt_metrics']['design_quality_score'],\n",
    "        'Organization': r['ppt_metrics']['organization_score']\n",
    "    } for r in results\n",
    "])\n",
    "\n",
    "# Plot RAG metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(df_rag))\n",
    "width = 0.2\n",
    "\n",
    "plt.bar(x - 1.5*width, df_rag['Context Relevance'], width, label='Context Relevance')\n",
    "plt.bar(x - 0.5*width, df_rag['Answer Correctness'], width, label='Answer Correctness')\n",
    "plt.bar(x + 0.5*width, df_rag['Answer Completeness'], width, label='Answer Completeness')\n",
    "plt.bar(x + 1.5*width, df_rag['Hallucination Score'], width, label='Hallucination Score')\n",
    "\n",
    "plt.xlabel('Queries')\n",
    "plt.ylabel('Score (0-10)')\n",
    "plt.title('RAG Agent Evaluation Metrics')\n",
    "plt.xticks(x, [f\"Query {i+1}\" for i in range(len(df_rag))])\n",
    "plt.legend()\n",
    "plt.ylim(0, 10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot PPT metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(df_ppt))\n",
    "width = 0.2\n",
    "\n",
    "plt.bar(x - width, df_ppt['Content Coverage'], width, label='Content Coverage')\n",
    "plt.bar(x, df_ppt['Design Quality'], width, label='Design Quality')\n",
    "plt.bar(x + width, df_ppt['Organization'], width, label='Organization')\n",
    "\n",
    "plt.xlabel('Queries')\n",
    "plt.ylabel('Score (0-10)')\n",
    "plt.title('PowerPoint Generation Evaluation Metrics')\n",
    "plt.xticks(x, [f\"Query {i+1}\" for i in range(len(df_ppt))])\n",
    "plt.legend()\n",
    "plt.ylim(0, 10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PowerPoint evaluator\n",
    "ppt_evaluator = PowerPointEvaluator()\n",
    "\n",
    "# Define a query to evaluate\n",
    "ppt_query = \"What are the main hypotheses in How Much Information?\"\n",
    "\n",
    "# Get context for the presentation\n",
    "context_docs = db_handler.get_documents_from_query(ppt_query, k=2)\n",
    "\n",
    "# Run the evaluation\n",
    "print(\"Generating PowerPoint presentation. This may take a minute...\")\n",
    "ppt_metrics = ppt_evaluator.evaluate_ppt_generation(ppt_query, context_docs)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"\\n=== PowerPoint Evaluation Results ===\")\n",
    "print(f\"Generation Time: {ppt_metrics.generation_time:.2f} seconds\")\n",
    "print(f\"Slides Count: {ppt_metrics.slides_count}\")\n",
    "print(f\"Avg. Content per Slide: {ppt_metrics.avg_content_per_slide:.1f} characters\")\n",
    "print(f\"Content Coverage: {ppt_metrics.content_coverage_score:.1f}/10\")\n",
    "print(f\"Design Quality: {ppt_metrics.design_quality_score:.1f}/10\")\n",
    "print(f\"Organization: {ppt_metrics.organization_score:.1f}/10\")\n",
    "\n",
    "# If you want to see the presentation file info\n",
    "import os\n",
    "if os.path.exists(\"C:/Users/lukas/Documents/Projects/tales/presentation.pptx\"):\n",
    "    file_size = os.path.getsize(\"C:/Users/lukas/Documents/Projects/tales/presentation.pptx\") / 1024  # KB\n",
    "    print(f\"\\nPresentation file size: {file_size:.1f} KB\")\n",
    "    print(\"Presentation saved at: C:/Users/lukas/Documents/Projects/tales/presentation.pptx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063551dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PowerPoint metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a simple bar chart for PowerPoint metrics\n",
    "metrics = [\n",
    "    ppt_metrics.content_coverage_score,\n",
    "    ppt_metrics.design_quality_score,\n",
    "    ppt_metrics.organization_score\n",
    "]\n",
    "metric_labels = ['Content Coverage', 'Design Quality', 'Organization']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(metric_labels, metrics, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{height:.1f}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.ylim(0, 10)  # Set y-axis from 0 to 10\n",
    "plt.axhline(y=5, color='gray', linestyle='--', alpha=0.5)  # Add a reference line at 5\n",
    "plt.title('PowerPoint Generation Evaluation Metrics', fontsize=16)\n",
    "plt.ylabel('Score (0-10)', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add a note about slide count\n",
    "plt.figtext(0.5, 0.01, f'Presentation contains {ppt_metrics.slides_count} slides | Generated in {ppt_metrics.generation_time:.1f} seconds',\n",
    "          ha='center', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a8f03",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to evaluate both:\n",
    "\n",
    "1. RAG (Retrieval-Augmented Generation) capabilities - evaluating the quality of information retrieval and response generation\n",
    "2. PowerPoint generation capabilities - evaluating the quality of presentations generated from the same information\n",
    "\n",
    "These evaluations provide insights into how well the system performs in both textual information delivery and visual presentation formats.\n",
    "\n",
    "To extend this evaluation:\n",
    "- Try different types of queries\n",
    "- Test with different documents in the vector store\n",
    "- Compare different PowerPoint styling approaches\n",
    "- Implement batch evaluation across multiple queries\n",
    "- Save evaluation results to track improvements over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
