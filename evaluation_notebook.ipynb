{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed85166",
   "metadata": {},
   "source": [
    "# Tales RAG and PowerPoint Agent Evaluation\n",
    "\n",
    "This notebook demonstrates how to evaluate the RAG agent and PowerPoint generation capabilities of the Tales system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d2a197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling RAG Agent...\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from tales.evaluation import RAGEvaluator, PowerPointEvaluator, run_batch_evaluation\n",
    "from tales.agent import agent\n",
    "from tales.db_handler import ChromaDBHandler\n",
    "from tales.config import DB_PATH\n",
    "from tales.utils import get_available_docs\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f12191",
   "metadata": {},
   "source": [
    "## Check Available Documents\n",
    "\n",
    "First, let's check what documents are available in our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf7dba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Initialized Successfully...\n",
      "Found 2 documents in the vector store:\n",
      " - data/How Much Information.pdf\n",
      " - data/bugra.pdf\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChromaDB handler\n",
    "db_handler = ChromaDBHandler(persist_directory=DB_PATH)\n",
    "\n",
    "# Get stored documents\n",
    "stored_docs = db_handler.get_stored_documents()\n",
    "print(f\"Found {len(stored_docs)} documents in the vector store:\")\n",
    "for doc in stored_docs:\n",
    "    print(f\" - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91121693",
   "metadata": {},
   "source": [
    "## Single Query Evaluation\n",
    "\n",
    "Let's evaluate the RAG agent on a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc54473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Initialized Successfully...\n",
      "Analyzing query...\n",
      "Retrieving documents...\n",
      "Generating answer...\n",
      "Reflecting on answer...\n",
      "=== RAG Evaluation Results ===\n",
      "Response Time: 3.96 seconds\n",
      "Context Relevance: 0.0/10\n",
      "Answer Correctness: 5.0/10\n",
      "Answer Completeness: 10.0/10\n",
      "Hallucination Score: 5.0/10 (lower is better)\n",
      "Documents Retrieved: 0\n",
      "Research Iterations: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG evaluator\n",
    "rag_evaluator = RAGEvaluator()\n",
    "\n",
    "# Define a query to evaluate\n",
    "query = \"What are the 3 hypothesis mentioned?\"\n",
    "\n",
    "# Run the evaluation\n",
    "rag_metrics, messages = rag_evaluator.evaluate_rag_query(query)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"=== RAG Evaluation Results ===\")\n",
    "print(f\"Response Time: {rag_metrics.response_time:.2f} seconds\")\n",
    "print(f\"Context Relevance: {rag_metrics.context_relevance_score:.1f}/10\")\n",
    "print(f\"Answer Correctness: {rag_metrics.answer_correctness_score:.1f}/10\")\n",
    "print(f\"Answer Completeness: {rag_metrics.answer_completeness_score:.1f}/10\")\n",
    "print(f\"Hallucination Score: {rag_metrics.hallucination_score:.1f}/10 (lower is better)\")\n",
    "print(f\"Documents Retrieved: {rag_metrics.num_documents_retrieved}\")\n",
    "print(f\"Research Iterations: {rag_metrics.num_research_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26924ae",
   "metadata": {},
   "source": [
    "## View the RAG Response\n",
    "\n",
    "Let's look at the response the RAG agent provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fdf4024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query:** What are the 3 hypothesis mentioned?\n",
       "\n",
       "**Response:**\n",
       "The three hypotheses mentioned in the text are:\n",
       "\n",
       "*   **H1.** Trust is lower if expectations are violated.\n",
       "*   **H2.** Changes in interface transparency affect trust depending on whether expectations are violated.\n",
       "*   **H3.** If expectations are violated, procedural transparency increases trust, but additional information about outcomes erodes this trust."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the last message (the response)\n",
    "from IPython.display import Markdown\n",
    "\n",
    "response = next((msg.content for msg in reversed(messages) if hasattr(msg, 'content')), \"No response found\")\n",
    "display(Markdown(f\"**Query:** {query}\\n\\n**Response:**\\n{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea090eb1",
   "metadata": {},
   "source": [
    "## PowerPoint Generation Evaluation\n",
    "\n",
    "Now, let's evaluate the PowerPoint generation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729cc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PowerPoint evaluator\n",
    "ppt_evaluator = PowerPointEvaluator()\n",
    "\n",
    "# Generate and evaluate a PowerPoint\n",
    "try:\n",
    "    ppt_metrics = await ppt_evaluator.evaluate_ppt_generation(messages)\n",
    "    \n",
    "    # Print PowerPoint metrics\n",
    "    print(\"=== PowerPoint Evaluation Results ===\")\n",
    "    print(f\"Generation Time: {ppt_metrics.generation_time:.2f} seconds\")\n",
    "    print(f\"Number of Slides: {ppt_metrics.slides_count}\")\n",
    "    print(f\"Avg Content Per Slide: {ppt_metrics.avg_content_per_slide:.1f} characters\")\n",
    "    print(f\"Content Coverage: {ppt_metrics.content_coverage_score:.1f}/10\")\n",
    "    print(f\"Design Quality: {ppt_metrics.design_quality_score:.1f}/10\")\n",
    "    print(f\"Organization: {ppt_metrics.organization_score:.1f}/10\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating PowerPoint: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13acdc",
   "metadata": {},
   "source": [
    "## Batch Evaluation\n",
    "\n",
    "Let's run a batch evaluation on multiple queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of queries\n",
    "queries = [\n",
    "    \"What are the key concepts in information theory?\",\n",
    "    \"Explain the challenges of data overload in modern society\",\n",
    "    \"What are the main differences between structured and unstructured data?\"\n",
    "]\n",
    "\n",
    "# Run batch evaluation (this may take some time)\n",
    "results = run_batch_evaluation(queries, save_path=\"notebook_evaluation_results.json\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nBatch Evaluation Summary:\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nQuery {i+1}: {result['query']}\")\n",
    "    print(f\"RAG Correctness: {result['rag_metrics']['answer_correctness_score']:.1f}/10\")\n",
    "    print(f\"RAG Completeness: {result['rag_metrics']['answer_completeness_score']:.1f}/10\")\n",
    "    print(f\"PowerPoint Slides: {result['ppt_metrics']['slides_count']}\")\n",
    "    print(f\"PowerPoint Content Coverage: {result['ppt_metrics']['content_coverage_score']:.1f}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9adae96",
   "metadata": {},
   "source": [
    "## Visualization of Results\n",
    "\n",
    "Let's create a simple visualization of the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_rag = pd.DataFrame([\n",
    "    {\n",
    "        'Query': r['query'],\n",
    "        'Context Relevance': r['rag_metrics']['context_relevance_score'],\n",
    "        'Answer Correctness': r['rag_metrics']['answer_correctness_score'],\n",
    "        'Answer Completeness': r['rag_metrics']['answer_completeness_score'],\n",
    "        'Hallucination Score': r['rag_metrics']['hallucination_score']\n",
    "    } for r in results\n",
    "])\n",
    "\n",
    "df_ppt = pd.DataFrame([\n",
    "    {\n",
    "        'Query': r['query'],\n",
    "        'Slides': r['ppt_metrics']['slides_count'],\n",
    "        'Content Coverage': r['ppt_metrics']['content_coverage_score'],\n",
    "        'Design Quality': r['ppt_metrics']['design_quality_score'],\n",
    "        'Organization': r['ppt_metrics']['organization_score']\n",
    "    } for r in results\n",
    "])\n",
    "\n",
    "# Plot RAG metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(df_rag))\n",
    "width = 0.2\n",
    "\n",
    "plt.bar(x - 1.5*width, df_rag['Context Relevance'], width, label='Context Relevance')\n",
    "plt.bar(x - 0.5*width, df_rag['Answer Correctness'], width, label='Answer Correctness')\n",
    "plt.bar(x + 0.5*width, df_rag['Answer Completeness'], width, label='Answer Completeness')\n",
    "plt.bar(x + 1.5*width, df_rag['Hallucination Score'], width, label='Hallucination Score')\n",
    "\n",
    "plt.xlabel('Queries')\n",
    "plt.ylabel('Score (0-10)')\n",
    "plt.title('RAG Agent Evaluation Metrics')\n",
    "plt.xticks(x, [f\"Query {i+1}\" for i in range(len(df_rag))])\n",
    "plt.legend()\n",
    "plt.ylim(0, 10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot PPT metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(df_ppt))\n",
    "width = 0.2\n",
    "\n",
    "plt.bar(x - width, df_ppt['Content Coverage'], width, label='Content Coverage')\n",
    "plt.bar(x, df_ppt['Design Quality'], width, label='Design Quality')\n",
    "plt.bar(x + width, df_ppt['Organization'], width, label='Organization')\n",
    "\n",
    "plt.xlabel('Queries')\n",
    "plt.ylabel('Score (0-10)')\n",
    "plt.title('PowerPoint Generation Evaluation Metrics')\n",
    "plt.xticks(x, [f\"Query {i+1}\" for i in range(len(df_ppt))])\n",
    "plt.legend()\n",
    "plt.ylim(0, 10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
