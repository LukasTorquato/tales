{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed85166",
   "metadata": {},
   "source": [
    "# Tales RAG and PowerPoint Agent Evaluation\n",
    "\n",
    "This notebook demonstrates how to evaluate the RAG agent and PowerPoint generation capabilities of the Tales system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d2a197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling RAG Agent...\n",
      "Database Initialized Successfully...\n"
     ]
    }
   ],
   "source": [
    "from tales.evaluation import RAGEvaluator\n",
    "from tales.db_handler import ChromaDBHandler\n",
    "from tales.config import DB_PATH\n",
    "\n",
    "# Initialize the RAG evaluator\n",
    "rag_evaluator = RAGEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f12191",
   "metadata": {},
   "source": [
    "## Check Available Documents\n",
    "\n",
    "First, let's check what documents are available in our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf7dba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Initialized Successfully...\n",
      "Found 2 documents in the vector store:\n",
      " - data/How Much Information.pdf\n",
      " - data/bugra.pdf\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChromaDB handler\n",
    "db_handler = ChromaDBHandler(persist_directory=DB_PATH)\n",
    "\n",
    "# Get stored documents\n",
    "stored_docs = db_handler.get_stored_documents()\n",
    "print(f\"Found {len(stored_docs)} documents in the vector store:\")\n",
    "for doc in stored_docs:\n",
    "    print(f\" - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91121693",
   "metadata": {},
   "source": [
    "## Single Query Evaluation\n",
    "\n",
    "Let's evaluate the RAG agent on a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc54473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing query...\n",
      "Retrieving documents...\n",
      "Generating answer...\n",
      "Reflecting on answer...\n",
      "{'context': [Document(id='07b92f5e-fa22-4eba-843f-ec47a4d003bf', metadata={'page': 0, 'page_label': '1', 'source': 'data/bugra.pdf'}, page_content='Bugra is a male student in National college of Ireland, he is 25 years old, he likes \\nelectronic music and is a addicted to drugs.'), Document(id='df3ca118-7f7c-44aa-976f-8a73a56539b5', metadata={'page': 0, 'page_label': '1', 'source': 'data/How Much Information.pdf'}, page_content='How Much Information?\\nEffects of Transparency on Trust in an Algorithmic Interface\\nRen´e F. Kizilcec\\nDepartment of Communication, Stanford University\\nkizilcec@stanford.edu\\nABSTRACT\\nThe rising prevalence of algorithmic interfaces, such as cu-\\nrated feeds in online news, raises new questions for designers,\\nscholars, and critics of media. This work focuses on how trans-\\nparent design of algorithmic interfaces can promote awareness\\nand foster trust. A two-stage process of how transparency\\naffects trust was hypothesized drawing on theories of infor-\\nmation processing and procedural justice. In an online ﬁeld\\nexperiment, three levels of system transparency were tested in\\nthe high-stakes context of peer assessment. Individuals whose\\nexpectations were violated (by receiving a lower grade than\\nexpected) trusted the system less, unless the grading algorithm\\nwas made more transparent through explanation. However,\\nproviding too much information eroded this trust. Attitudes\\nof individuals whose expectations were met did not vary with\\ntransparency. Results are discussed in terms of a dual process\\nmodel of attitude change and the depth of justiﬁcation of per-\\nceived inconsistency. Designing for trust requires balanced\\ninterface transparency—not too little and not too much.\\nACM Classiﬁcation Keywords\\nH.5.2. Information Interfaces and Presentation (e.g. HCI):\\nUser Interfaces; K.3.1. Computers and Education: Computer\\nUses in Education.\\nAuthor Keywords\\nInterface Design; Algorithm Awareness; Attitude Change;'), Document(id='abcdcab5-aa1e-4598-8dd4-f7ba3ecfd97c', metadata={'page': 0, 'page_label': '1', 'source': 'data/How Much Information.pdf'}, page_content='promote or erode users’ trust in a system by changing beliefs\\nabout its trustworthiness.\\nTrust is a key concern in the design of technology, as it affects\\nthe initial adoption and continued use of technologies [4, 24].\\nIn light of people’s tendency to treat new technologies as social\\nactors [23], the present deﬁnition of trust draws on prior work\\nin ofﬂine interpersonal contexts. Trust is understood as “an\\nattitude of conﬁdent expectation in an online situation of risk\\nthat one’s vulnerabilities will not be exploited” (p.740) [5].\\nOne way to assure individuals that they will not be exploited\\nis through transparency in design, which may foster a better\\nunderstanding of the system and the extent to which it is fair\\nand accurate.\\nEarly research on transparency in complex systems focused\\non explanations to expose the data or reasoning underlying\\na system’s output [11]. One of the ﬁrst artiﬁcial intelligence\\ninterfaces, MYCIN, provided explanations to help users un-\\nderstand its reasoning and instill conﬁdence [2]. Providing ex-\\nplanations can increase performance on information retrieval\\ntasks [13] and improve attitudes toward automated collabora-\\ntive ﬁltering [10]. An experiment in an e-commerce context\\nfound that complementing product recommendations with dif-\\nferent kinds of explanations positively inﬂuenced consumer\\nbeliefs [31]. In particular, ‘why’, ‘how’, and ‘trade-off’ expla-\\nnations raised perceptions of competence, benevolence, and\\nintegrity, respectively. Another experiment found ‘why’ expla-'), Document(id='ea05cc32-4ca4-45cf-bb59-794427c82346', metadata={'page': 4, 'page_label': '5', 'source': 'data/How Much Information.pdf'}, page_content='REFERENCES\\n1. Frank Bannister and Regina Connolly. 2011. The Trouble\\nwith Transparency: A Critical Review of Openness in\\ne-Government. Policy & Internet3, 1 (2011), 1–30.\\n2. Bruce G Buchanan, Edward Hance Shortliffe, and others.\\n1984. Rule-based expert systems. V ol. 3. Addison-Wesley\\nReading, MA.\\n3. Judee K Burgoon and Jerold L Hale. 1988. Nonverbal\\nexpectancy violations: Model elaboration and application\\nto immediacy behaviors.Communications Monographs\\n55, 1 (1988), 58–79.\\n4. Karen S Cook, Chris Snijders, Vincent Buskens, and\\nCoye Cheshire. 2009.eTrust: Forming relationships in\\nthe online world. Russell Sage Foundation.\\n5. Cynthia L Corritore, Beverly Kracher, and Susan\\nWiedenbeck. 2003. On-line trust: concepts, evolving\\nthemes, a model.International Journal of\\nHuman-Computer Studies58, 6 (2003), 737–758.\\n6. Henriette Cramer, Vanessa Evers, Satyan Ramlal,\\nMaarten Van Someren, Lloyd Rutledge, Natalia Stash,\\nLora Aroyo, and Bob Wielinga. 2008. The effects of\\ntransparency on trust in and acceptance of a\\ncontent-based art recommender.User Modeling and\\nUser-Adapted Interaction18, 5 (2008), 455–496.\\n7. Motahhare Eslami, Aimee Rickman, Kristen Vaccaro,\\nAmirhossein Aleyasen, Andy Vuong, Karrie Karahalios,\\nKevin Hamilton, and Christian Sandvig. 2015. “I always\\nassumed that I wasn’t really that close to [her]”:\\nReasoning about invisible algorithms in the news feed. In\\nProceedings of the 33rd Annual SIGCHI Conference on\\nHuman Factors in Computing Systems. 153–162.\\n8. Alyssa Glass, Deborah L McGuinness, and Michael'), Document(id='20450fd3-5bba-4e05-9ea2-5b6b0b4a51ce', metadata={'page': 1, 'page_label': '2', 'source': 'data/How Much Information.pdf'}, page_content='parent solution comforted the students at ﬁrst. However, when\\nthe instructor announced the original and adjusted grades, the\\nstudents were once again upset about it. What went wrong?\\nThree important constructs embedded in this narrative are trust\\nin the system, transparency (revealing the grading procedure),\\nand the violation of positive expectations (receiving unfair\\ngrades). In particular, there were three consecutive levels of\\ntransparency: no explanation, a purely procedural explanation,\\nand additionally providing data. Around the time that this\\ngrading issue occurred, there was a relevant development in\\npeer assessment practices in large online courses.\\nTRUST AND TRANSPARENCY IN PEER ASSESSMENT\\nOnline peer assessment provides a suitable context to study\\nthe effects of interface transparency on trust. It is a natural\\nenvironment with high stakes that parallels the context of the\\nmotivating anecdote, while the digital format enables random\\nassignment to different experimental conditions. Peer assess-\\nment is a proven method for scaling the grading of a large\\nnumber of assessments, such as is required in massive open\\nonline courses (MOOCs) [14, 19]. In peer assessment, every\\nperson evaluates several submissions by peers and has their\\nown submission evaluated by several peers. Peer grading is\\noften supplemented by self grading to encourage the devel-\\nopment of self-evaluative skills and is generally thought to\\n“augment student learning” [25]. Surveys found positive stu-'), Document(id='9b014e83-1f9a-4a4d-9fc0-b6d7acf265c0', metadata={'page': 0, 'page_label': '1', 'source': 'data/How Much Information.pdf'}, page_content='designers, scholars, and critics of media. The consequences\\nof increased algorithm awareness through more transparent\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\\non the ﬁrst page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission\\nand/or a fee. Request permissions from Permissions@acm.org.\\nCHI 2016, May 7–12, 2016, San Jose, CA, USA\\nCopyright is held by the owner/author(s). Publication rights licensed to ACM.\\nACM 978-1-4503-3362-7/16/05...$15.00\\nDOI http://dx.doi.org/10.1145/2858036.2858402\\ninterface design are not well understood, especially in real\\nworld situations where the stakes are high. Transparency may\\npromote or erode users’ trust in a system by changing beliefs\\nabout its trustworthiness.\\nTrust is a key concern in the design of technology, as it affects\\nthe initial adoption and continued use of technologies [4, 24].\\nIn light of people’s tendency to treat new technologies as social\\nactors [23], the present deﬁnition of trust draws on prior work\\nin ofﬂine interpersonal contexts. Trust is understood as “an\\nattitude of conﬁdent expectation in an online situation of risk'), Document(id='c34450e9-4353-495c-998b-31fcf17d943f', metadata={'page': 3, 'page_label': '4', 'source': 'data/How Much Information.pdf'}, page_content='ing increasingly difﬁcult to hide—the intelligence behind the\\nanti-lock breaking system is easier to hide than that behind a\\nself-driving car, for instance. In an algorithmic interface, the\\nright amount of system transparency—not too little and not\\ntoo much—can foster positive attitudes and encourage people\\nto reap the beneﬁts of intelligent technology.\\nACKNOWLEDGMENTS\\nI thank Clifford Nass for his inspiration and guidance, Chin-\\nmay Kulkarni for his help with the implementation, Daniel\\nMcFarland for providing access to his online course, and Emily\\nSchneider and the CHI reviewers for their helpful feedback.\\nCuration and Algorithms\\n#chi4good, CHI 2016, San Jose, CA, USA\\n2393'), Document(id='c9203298-8137-4b4a-83f0-1805655e2493', metadata={'page': 1, 'page_label': '2', 'source': 'data/How Much Information.pdf'}, page_content='ment is a proven method for scaling the grading of a large\\nnumber of assessments, such as is required in massive open\\nonline courses (MOOCs) [14, 19]. In peer assessment, every\\nperson evaluates several submissions by peers and has their\\nown submission evaluated by several peers. Peer grading is\\noften supplemented by self grading to encourage the devel-\\nopment of self-evaluative skills and is generally thought to\\n“augment student learning” [25]. Surveys found positive stu-\\ndent attitudes toward peer grading, but also some concern over\\nthe fairness and reliability of peer grades [27, 26, 12, 32].\\nTraditionally, the ﬁnal assessment grade is simply the mean or\\nmedian of the peer grades, which turns out to be similar to for-\\nmal instructor grades [25, 14]. Kulkarni and colleagues [14]\\ntested peer assessment in a MOOC and found that 66% of\\nmedian peer grades were within 10% of instructor grades.\\nPiech and colleagues [19] were able to improve accuracy by\\n30% using algorithms that adjust grades for grader bias and\\nreliability. Their “tuned models” of peer assessment were a\\nsubstantial improvement, but it was unclear how to commu-\\nnicate this to online learners. The details of the algorithm\\nwould be overwhelming and most learners were still under the\\nimpression that peer grades were simple averages. A social\\nmedia analogue of this issue is Facebook’s News Feed ranking\\nalgorithm that is designed to provide a better user experience\\nthan a chronological view. Yet many users are unaware of'), Document(id='6e862f56-0a6a-4d3e-a222-13069815d489', metadata={'page': 3, 'page_label': '4', 'source': 'data/How Much Information.pdf'}, page_content='tations of the current work include the small sample size, the\\nfocus on self-report outcomes, and the absence of qualitative\\ninterviews to gain deeper insights into the user experience.\\nFuture work should replicate the results in different contexts,\\nassess longitudinal behavioral outcomes, and investigate the\\nproposed mechanisms.\\nThe ongoing debate in the HCI community around system\\ntransparency [9], or seamless versus ‘seamful’ design, docu-\\nments the importance and complexity of the issue. Interface\\ndesign should foremost be tailored to its application context.\\nIn education, academic assessment is traditionally a highly\\nopaque system. New forms of data-enriched assessment pro-\\nvide novel challenges and opportunities for transparent design\\nin digital learning environments [30]. As with any new technol-\\nogy, its adoption and potential beneﬁts depend on individual\\nattitudes and public opinion. Intelligent technologies have\\nbeen around for some time, but their intelligence is becom-\\ning increasingly difﬁcult to hide—the intelligence behind the\\nanti-lock breaking system is easier to hide than that behind a\\nself-driving car, for instance. In an algorithmic interface, the\\nright amount of system transparency—not too little and not\\ntoo much—can foster positive attitudes and encourage people\\nto reap the beneﬁts of intelligent technology.\\nACKNOWLEDGMENTS\\nI thank Clifford Nass for his inspiration and guidance, Chin-\\nmay Kulkarni for his help with the implementation, Daniel'), Document(id='93d61c27-ca35-452c-9a4b-0f3a0a7c322a', metadata={'page': 2, 'page_label': '3', 'source': 'data/How Much Information.pdf'}, page_content='assessment system. Four items assessed facets of trust (i.e.,\\nattitude of conﬁdent expectation that one’s vulnerabilities will\\nnot be exploited): ‘To what extent do you understand how your\\ngrade is computed in peer grading?’; ‘How fair or unfair was\\nthe peer grading process?’; ‘How accurate or inaccurate was\\nthe peer grading process?’; and ‘How much did you trust or dis-\\ntrust your peers to grade you fairly?’. Participants responded\\non construct-speciﬁc and fully labeled response scales with\\n5 points for the unipolar item about understanding (‘No un-\\nderstanding at all’ to ‘Excellent understanding’), and 7 points\\nfor all other items (e.g., ‘Deﬁnitely fair’ to ‘Deﬁnitely unfair’).\\nAs expected, ratings of system understanding, fairness, accu-\\nracy, and trust in peers’ fair grading were highly correlated\\n(Cronbach’sα = 0.83). They were combined by simple av-\\neraging into an index that measured each participant’s trust\\nin the system (M = 3.55, SD = 1.13, range from 0 to 6). In\\naddition, each participant’s self-assessment grade (self grade)\\nand adjusted peer grade before late submission penalties (peer\\ngrade) were available. The difference between the two grades\\nserved as a measure of expectation violation, as either a binary\\n(Figure 1) or continuous variable (Figure 2). In the binary\\ncase, expectation violation was deﬁned as a self grade that was\\nover 2 points above the peer grade.\\nRESULTS\\nFigure 1 shows the average trust index for participants who\\neither received a grade that matched their expectations or one'), Document(id='d363c07c-d1c0-49f9-8ae7-eca3ca2f9d1b', metadata={'page': 0, 'page_label': '1', 'source': 'data/How Much Information.pdf'}, page_content='planations can increase performance on information retrieval\\ntasks [13] and improve attitudes toward automated collabora-\\ntive ﬁltering [10]. An experiment in an e-commerce context\\nfound that complementing product recommendations with dif-\\nferent kinds of explanations positively inﬂuenced consumer\\nbeliefs [31]. In particular, ‘why’, ‘how’, and ‘trade-off’ expla-\\nnations raised perceptions of competence, benevolence, and\\nintegrity, respectively. Another experiment found ‘why’ expla-\\nnations to increase recommendation acceptance but not trust in\\nthe system [6]. The evidence suggests that added explanations\\ncan promote positive attitudes toward a system, but not neces-\\nsarily trust. In a qualitative study of factors inﬂuencing trust\\nin a complex adaptive agent, system transparency emerged as\\na core theme in user interviews [8]. Increased transparency\\nis also associated with fewer misconceptions [16] and higher\\nconﬁdence in system recommendations [29]. However, an\\nexperimental test of increased transparency in an e-commerce\\nsystem found no gains in trust or perceived competence [21].\\nFinally, a study of the Facebook News Feed found that in-\\ncreased algorithm awareness may not raise satisfaction, though\\nit can promote engagement with the service [7].\\nThe available evidence on how transparency affects trust is\\nmixed—some studies found positive effects, while others\\nfound no effect. The literature offers few rigorous experimen-\\ntal tests, and the ones reviewed above took place in controlled'), Document(id='5887bd6a-644c-4286-bcff-c30dccec90d3', metadata={'page': 1, 'page_label': '2', 'source': 'data/How Much Information.pdf'}, page_content='present study addresses a number of these shortcomings by\\ntesting the effects of transparency in a natural and high-stakes\\nenvironment. Additionally, the current experiment compares\\nbetween three levels of transparency (low, medium, and high)\\nand evaluates the moderating role of expectation violation, the\\nextent to which the system output matches user expectations.\\nA MOTIVATING ANECDOTE\\nA true story inspired this research and informed the study de-\\nsign and hypotheses. In a large, in-person HCI class, some\\nstudents noticed that they received lower homework grades\\nthan their peers who were in a discussion section with a differ-\\nent teaching assistant (TA). What happened was that each TA\\nhad graded all homework questions for a subset of students, re-\\nsulting in inconsistent grading between TAs. Students who got\\nharsher graders were naturally more upset. To resolve the is-\\nsue, the instructor informed students that their grades would be\\nstatistically adjusted for this bias to make grades fair. This ap-\\nparent solution comforted the students at ﬁrst. However, when\\nthe instructor announced the original and adjusted grades, the\\nstudents were once again upset about it. What went wrong?\\nThree important constructs embedded in this narrative are trust\\nin the system, transparency (revealing the grading procedure),\\nand the violation of positive expectations (receiving unfair\\ngrades). In particular, there were three consecutive levels of\\ntransparency: no explanation, a purely procedural explanation,')], 'query': AIMessage(content='Who is Bugra?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--edb33066-70b3-4c8a-8aec-047b181011ac-0', usage_metadata={'input_tokens': 83, 'output_tokens': 6, 'total_tokens': 89, 'input_token_details': {'cache_read': 0}}), 'more_research': False, 'messages': [HumanMessage(content='Who is Bugra?', additional_kwargs={}, response_metadata={}, id='e033b985-43e5-4a8f-8c38-69d5a5a0abc2'), AIMessage(content='Bugra is a 25-year-old male student at the National College of Ireland who enjoys electronic music and is addicted to drugs.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--3473238b-db01-4b1b-8fb4-f5bbc1421a03-0', usage_metadata={'input_tokens': 3704, 'output_tokens': 30, 'total_tokens': 3734, 'input_token_details': {'cache_read': 0}})]}\n",
      "Query:  Who is Bugra?\n",
      "Response:  Bugra is a 25-year-old male student at the National College of Ireland who enjoys electronic music and is addicted to drugs.\n",
      "Context:  ['Bugra is a male student in National college of Ireland, he is 25 years old, he likes \\nelectronic music and is a addicted to drugs.', 'How Much Information?\\nEffects of Transparency on Trust in an Algorithmic Interface\\nRen´e F. Kizilcec\\nDepartment of Communication, Stanford University\\nkizilcec@stanford.edu\\nABSTRACT\\nThe rising prevalence of algorithmic interfaces, such as cu-\\nrated feeds in online news, raises new questions for designers,\\nscholars, and critics of media. This work focuses on how trans-\\nparent design of algorithmic interfaces can promote awareness\\nand foster trust. A two-stage process of how transparency\\naffects trust was hypothesized drawing on theories of infor-\\nmation processing and procedural justice. In an online ﬁeld\\nexperiment, three levels of system transparency were tested in\\nthe high-stakes context of peer assessment. Individuals whose\\nexpectations were violated (by receiving a lower grade than\\nexpected) trusted the system less, unless the grading algorithm\\nwas made more transparent through explanation. However,\\nproviding too much information eroded this trust. Attitudes\\nof individuals whose expectations were met did not vary with\\ntransparency. Results are discussed in terms of a dual process\\nmodel of attitude change and the depth of justiﬁcation of per-\\nceived inconsistency. Designing for trust requires balanced\\ninterface transparency—not too little and not too much.\\nACM Classiﬁcation Keywords\\nH.5.2. Information Interfaces and Presentation (e.g. HCI):\\nUser Interfaces; K.3.1. Computers and Education: Computer\\nUses in Education.\\nAuthor Keywords\\nInterface Design; Algorithm Awareness; Attitude Change;', 'promote or erode users’ trust in a system by changing beliefs\\nabout its trustworthiness.\\nTrust is a key concern in the design of technology, as it affects\\nthe initial adoption and continued use of technologies [4, 24].\\nIn light of people’s tendency to treat new technologies as social\\nactors [23], the present deﬁnition of trust draws on prior work\\nin ofﬂine interpersonal contexts. Trust is understood as “an\\nattitude of conﬁdent expectation in an online situation of risk\\nthat one’s vulnerabilities will not be exploited” (p.740) [5].\\nOne way to assure individuals that they will not be exploited\\nis through transparency in design, which may foster a better\\nunderstanding of the system and the extent to which it is fair\\nand accurate.\\nEarly research on transparency in complex systems focused\\non explanations to expose the data or reasoning underlying\\na system’s output [11]. One of the ﬁrst artiﬁcial intelligence\\ninterfaces, MYCIN, provided explanations to help users un-\\nderstand its reasoning and instill conﬁdence [2]. Providing ex-\\nplanations can increase performance on information retrieval\\ntasks [13] and improve attitudes toward automated collabora-\\ntive ﬁltering [10]. An experiment in an e-commerce context\\nfound that complementing product recommendations with dif-\\nferent kinds of explanations positively inﬂuenced consumer\\nbeliefs [31]. In particular, ‘why’, ‘how’, and ‘trade-off’ expla-\\nnations raised perceptions of competence, benevolence, and\\nintegrity, respectively. Another experiment found ‘why’ expla-', 'REFERENCES\\n1. Frank Bannister and Regina Connolly. 2011. The Trouble\\nwith Transparency: A Critical Review of Openness in\\ne-Government. Policy & Internet3, 1 (2011), 1–30.\\n2. Bruce G Buchanan, Edward Hance Shortliffe, and others.\\n1984. Rule-based expert systems. V ol. 3. Addison-Wesley\\nReading, MA.\\n3. Judee K Burgoon and Jerold L Hale. 1988. Nonverbal\\nexpectancy violations: Model elaboration and application\\nto immediacy behaviors.Communications Monographs\\n55, 1 (1988), 58–79.\\n4. Karen S Cook, Chris Snijders, Vincent Buskens, and\\nCoye Cheshire. 2009.eTrust: Forming relationships in\\nthe online world. Russell Sage Foundation.\\n5. Cynthia L Corritore, Beverly Kracher, and Susan\\nWiedenbeck. 2003. On-line trust: concepts, evolving\\nthemes, a model.International Journal of\\nHuman-Computer Studies58, 6 (2003), 737–758.\\n6. Henriette Cramer, Vanessa Evers, Satyan Ramlal,\\nMaarten Van Someren, Lloyd Rutledge, Natalia Stash,\\nLora Aroyo, and Bob Wielinga. 2008. The effects of\\ntransparency on trust in and acceptance of a\\ncontent-based art recommender.User Modeling and\\nUser-Adapted Interaction18, 5 (2008), 455–496.\\n7. Motahhare Eslami, Aimee Rickman, Kristen Vaccaro,\\nAmirhossein Aleyasen, Andy Vuong, Karrie Karahalios,\\nKevin Hamilton, and Christian Sandvig. 2015. “I always\\nassumed that I wasn’t really that close to [her]”:\\nReasoning about invisible algorithms in the news feed. In\\nProceedings of the 33rd Annual SIGCHI Conference on\\nHuman Factors in Computing Systems. 153–162.\\n8. Alyssa Glass, Deborah L McGuinness, and Michael', 'parent solution comforted the students at ﬁrst. However, when\\nthe instructor announced the original and adjusted grades, the\\nstudents were once again upset about it. What went wrong?\\nThree important constructs embedded in this narrative are trust\\nin the system, transparency (revealing the grading procedure),\\nand the violation of positive expectations (receiving unfair\\ngrades). In particular, there were three consecutive levels of\\ntransparency: no explanation, a purely procedural explanation,\\nand additionally providing data. Around the time that this\\ngrading issue occurred, there was a relevant development in\\npeer assessment practices in large online courses.\\nTRUST AND TRANSPARENCY IN PEER ASSESSMENT\\nOnline peer assessment provides a suitable context to study\\nthe effects of interface transparency on trust. It is a natural\\nenvironment with high stakes that parallels the context of the\\nmotivating anecdote, while the digital format enables random\\nassignment to different experimental conditions. Peer assess-\\nment is a proven method for scaling the grading of a large\\nnumber of assessments, such as is required in massive open\\nonline courses (MOOCs) [14, 19]. In peer assessment, every\\nperson evaluates several submissions by peers and has their\\nown submission evaluated by several peers. Peer grading is\\noften supplemented by self grading to encourage the devel-\\nopment of self-evaluative skills and is generally thought to\\n“augment student learning” [25]. Surveys found positive stu-', 'designers, scholars, and critics of media. The consequences\\nof increased algorithm awareness through more transparent\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\\non the ﬁrst page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission\\nand/or a fee. Request permissions from Permissions@acm.org.\\nCHI 2016, May 7–12, 2016, San Jose, CA, USA\\nCopyright is held by the owner/author(s). Publication rights licensed to ACM.\\nACM 978-1-4503-3362-7/16/05...$15.00\\nDOI http://dx.doi.org/10.1145/2858036.2858402\\ninterface design are not well understood, especially in real\\nworld situations where the stakes are high. Transparency may\\npromote or erode users’ trust in a system by changing beliefs\\nabout its trustworthiness.\\nTrust is a key concern in the design of technology, as it affects\\nthe initial adoption and continued use of technologies [4, 24].\\nIn light of people’s tendency to treat new technologies as social\\nactors [23], the present deﬁnition of trust draws on prior work\\nin ofﬂine interpersonal contexts. Trust is understood as “an\\nattitude of conﬁdent expectation in an online situation of risk', 'ing increasingly difﬁcult to hide—the intelligence behind the\\nanti-lock breaking system is easier to hide than that behind a\\nself-driving car, for instance. In an algorithmic interface, the\\nright amount of system transparency—not too little and not\\ntoo much—can foster positive attitudes and encourage people\\nto reap the beneﬁts of intelligent technology.\\nACKNOWLEDGMENTS\\nI thank Clifford Nass for his inspiration and guidance, Chin-\\nmay Kulkarni for his help with the implementation, Daniel\\nMcFarland for providing access to his online course, and Emily\\nSchneider and the CHI reviewers for their helpful feedback.\\nCuration and Algorithms\\n#chi4good, CHI 2016, San Jose, CA, USA\\n2393', 'ment is a proven method for scaling the grading of a large\\nnumber of assessments, such as is required in massive open\\nonline courses (MOOCs) [14, 19]. In peer assessment, every\\nperson evaluates several submissions by peers and has their\\nown submission evaluated by several peers. Peer grading is\\noften supplemented by self grading to encourage the devel-\\nopment of self-evaluative skills and is generally thought to\\n“augment student learning” [25]. Surveys found positive stu-\\ndent attitudes toward peer grading, but also some concern over\\nthe fairness and reliability of peer grades [27, 26, 12, 32].\\nTraditionally, the ﬁnal assessment grade is simply the mean or\\nmedian of the peer grades, which turns out to be similar to for-\\nmal instructor grades [25, 14]. Kulkarni and colleagues [14]\\ntested peer assessment in a MOOC and found that 66% of\\nmedian peer grades were within 10% of instructor grades.\\nPiech and colleagues [19] were able to improve accuracy by\\n30% using algorithms that adjust grades for grader bias and\\nreliability. Their “tuned models” of peer assessment were a\\nsubstantial improvement, but it was unclear how to commu-\\nnicate this to online learners. The details of the algorithm\\nwould be overwhelming and most learners were still under the\\nimpression that peer grades were simple averages. A social\\nmedia analogue of this issue is Facebook’s News Feed ranking\\nalgorithm that is designed to provide a better user experience\\nthan a chronological view. Yet many users are unaware of', 'tations of the current work include the small sample size, the\\nfocus on self-report outcomes, and the absence of qualitative\\ninterviews to gain deeper insights into the user experience.\\nFuture work should replicate the results in different contexts,\\nassess longitudinal behavioral outcomes, and investigate the\\nproposed mechanisms.\\nThe ongoing debate in the HCI community around system\\ntransparency [9], or seamless versus ‘seamful’ design, docu-\\nments the importance and complexity of the issue. Interface\\ndesign should foremost be tailored to its application context.\\nIn education, academic assessment is traditionally a highly\\nopaque system. New forms of data-enriched assessment pro-\\nvide novel challenges and opportunities for transparent design\\nin digital learning environments [30]. As with any new technol-\\nogy, its adoption and potential beneﬁts depend on individual\\nattitudes and public opinion. Intelligent technologies have\\nbeen around for some time, but their intelligence is becom-\\ning increasingly difﬁcult to hide—the intelligence behind the\\nanti-lock breaking system is easier to hide than that behind a\\nself-driving car, for instance. In an algorithmic interface, the\\nright amount of system transparency—not too little and not\\ntoo much—can foster positive attitudes and encourage people\\nto reap the beneﬁts of intelligent technology.\\nACKNOWLEDGMENTS\\nI thank Clifford Nass for his inspiration and guidance, Chin-\\nmay Kulkarni for his help with the implementation, Daniel', 'assessment system. Four items assessed facets of trust (i.e.,\\nattitude of conﬁdent expectation that one’s vulnerabilities will\\nnot be exploited): ‘To what extent do you understand how your\\ngrade is computed in peer grading?’; ‘How fair or unfair was\\nthe peer grading process?’; ‘How accurate or inaccurate was\\nthe peer grading process?’; and ‘How much did you trust or dis-\\ntrust your peers to grade you fairly?’. Participants responded\\non construct-speciﬁc and fully labeled response scales with\\n5 points for the unipolar item about understanding (‘No un-\\nderstanding at all’ to ‘Excellent understanding’), and 7 points\\nfor all other items (e.g., ‘Deﬁnitely fair’ to ‘Deﬁnitely unfair’).\\nAs expected, ratings of system understanding, fairness, accu-\\nracy, and trust in peers’ fair grading were highly correlated\\n(Cronbach’sα = 0.83). They were combined by simple av-\\neraging into an index that measured each participant’s trust\\nin the system (M = 3.55, SD = 1.13, range from 0 to 6). In\\naddition, each participant’s self-assessment grade (self grade)\\nand adjusted peer grade before late submission penalties (peer\\ngrade) were available. The difference between the two grades\\nserved as a measure of expectation violation, as either a binary\\n(Figure 1) or continuous variable (Figure 2). In the binary\\ncase, expectation violation was deﬁned as a self grade that was\\nover 2 points above the peer grade.\\nRESULTS\\nFigure 1 shows the average trust index for participants who\\neither received a grade that matched their expectations or one', 'planations can increase performance on information retrieval\\ntasks [13] and improve attitudes toward automated collabora-\\ntive ﬁltering [10]. An experiment in an e-commerce context\\nfound that complementing product recommendations with dif-\\nferent kinds of explanations positively inﬂuenced consumer\\nbeliefs [31]. In particular, ‘why’, ‘how’, and ‘trade-off’ expla-\\nnations raised perceptions of competence, benevolence, and\\nintegrity, respectively. Another experiment found ‘why’ expla-\\nnations to increase recommendation acceptance but not trust in\\nthe system [6]. The evidence suggests that added explanations\\ncan promote positive attitudes toward a system, but not neces-\\nsarily trust. In a qualitative study of factors inﬂuencing trust\\nin a complex adaptive agent, system transparency emerged as\\na core theme in user interviews [8]. Increased transparency\\nis also associated with fewer misconceptions [16] and higher\\nconﬁdence in system recommendations [29]. However, an\\nexperimental test of increased transparency in an e-commerce\\nsystem found no gains in trust or perceived competence [21].\\nFinally, a study of the Facebook News Feed found that in-\\ncreased algorithm awareness may not raise satisfaction, though\\nit can promote engagement with the service [7].\\nThe available evidence on how transparency affects trust is\\nmixed—some studies found positive effects, while others\\nfound no effect. The literature offers few rigorous experimen-\\ntal tests, and the ones reviewed above took place in controlled', 'present study addresses a number of these shortcomings by\\ntesting the effects of transparency in a natural and high-stakes\\nenvironment. Additionally, the current experiment compares\\nbetween three levels of transparency (low, medium, and high)\\nand evaluates the moderating role of expectation violation, the\\nextent to which the system output matches user expectations.\\nA MOTIVATING ANECDOTE\\nA true story inspired this research and informed the study de-\\nsign and hypotheses. In a large, in-person HCI class, some\\nstudents noticed that they received lower homework grades\\nthan their peers who were in a discussion section with a differ-\\nent teaching assistant (TA). What happened was that each TA\\nhad graded all homework questions for a subset of students, re-\\nsulting in inconsistent grading between TAs. Students who got\\nharsher graders were naturally more upset. To resolve the is-\\nsue, the instructor informed students that their grades would be\\nstatistically adjusted for this bias to make grades fair. This ap-\\nparent solution comforted the students at ﬁrst. However, when\\nthe instructor announced the original and adjusted grades, the\\nstudents were once again upset about it. What went wrong?\\nThree important constructs embedded in this narrative are trust\\nin the system, transparency (revealing the grading procedure),\\nand the violation of positive expectations (receiving unfair\\ngrades). In particular, there were three consecutive levels of\\ntransparency: no explanation, a purely procedural explanation,']\n",
      "=== RAG Evaluation Results ===\n",
      "Response Time: 3.44 seconds\n",
      "Context Relevance: 0.0/10\n",
      "Answer Correctness: 10.0/10\n",
      "Answer Completeness: 10.0/10\n",
      "Hallucination Score: 0.0/10 (lower is better)\n",
      "Documents Retrieved: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a query to evaluate\n",
    "query = \"Who is Bugra?\"\n",
    "\n",
    "# Run the evaluation\n",
    "rag_metrics, messages = rag_evaluator.evaluate_rag_query(query)\n",
    "\n",
    "# Print the metrics\"\n",
    "print(\"=== RAG Evaluation Results ===\")\n",
    "print(f\"Response Time: {rag_metrics.response_time:.2f} seconds\")\n",
    "print(f\"Context Relevance: {rag_metrics.context_relevance_score:.1f}/10\")\n",
    "print(f\"Answer Correctness: {rag_metrics.answer_correctness_score:.1f}/10\")\n",
    "print(f\"Answer Completeness: {rag_metrics.answer_completeness_score:.1f}/10\")\n",
    "print(f\"Hallucination Score: {rag_metrics.hallucination_score:.1f}/10 (lower is better)\")\n",
    "print(f\"Documents Retrieved: {rag_metrics.num_documents_retrieved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26924ae",
   "metadata": {},
   "source": [
    "## View the RAG Response\n",
    "\n",
    "Let's look at the response the RAG agent provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fdf4024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query:** Who is Bugra?\n",
       "\n",
       "**Response:**\n",
       "Bugra is a 25-year-old male student at the National College of Ireland who enjoys electronic music and is addicted to drugs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the last message (the response)\n",
    "from IPython.display import Markdown\n",
    "\n",
    "response = next((msg.content for msg in reversed(messages) if hasattr(msg, 'content')), \"No response found\")\n",
    "display(Markdown(f\"**Query:** {query}\\n\\n**Response:**\\n{response}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
